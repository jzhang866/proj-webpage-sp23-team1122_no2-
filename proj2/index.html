<style>

	*{
		font-family: "Times New Roman";
	}

	h1{
		font-size: 70px;
	}

	h2{
		font-size: 40px;
	}

	p{
		font-size: 20px;
	}

	ul{
		font-size: 30px;
	}

	body {
		color: black;
	}

	.task_img {
		width: 800px;
		margin: 20px;
	}

	.task3 {
		height: 500px;
		width:  500px;
		margin: 20px;
	}

</style>

<html>
<head>
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body style="padding: 20PX;">
<div class="container text-center">
	<h1 id = "project-1-rasterizer">Project 2: MeshEdit</h1>
	<p>Partners: Jinghua Zhang, Zhiwei Zheng</p>
	<p>Here is the link to our pages: <a href="https://jzhang866.github.io/proj-webpage-sp23-team1122_no2-/">https://jzhang866.github.io/proj-webpage-sp23-team1122_no2-/</a></p>
</div>
<br>

<div class="container">
	<h2 id="overview">Overview</h2>
	<p>
		In assignment 1, we have implemented some rasterize tools for graphics renderers to draw SVG files. We have accomplished basic rasterizer with antialiasing methods to
		draw triangles smoothly with less jaggles, texture mapping to apply pixel sampling onto screen, and transformation coordination for reshaping the objects. Overall, assignment 1 allows us to familiarize ourselves with basic graphic rendering techniques.
	</p>
</div>

<div class="container">
	<h2 id="task1">Part 1</h2>
	<p>
		1.1 De Casteljau's algorithm can be used to get the point on a Bezier curve based on the given t which belongs to [0, 1]. Additionally, it's a recursive algorithm as
		it recursively does lerp to finally obtain the corresponding point. In this assignment, to implement de Casteljou's algorithm, we only need to achieve the function named evaluateStep in BezierCurve class.
		Let's say we have n points, then we need to do the lerp to get n-1 intermediate points based on intermedia_points[i] = (1-t) * points[i] + t * points[i+1], according to the given t. The function would be
		called again and again until we only get one point, which is taken as the point on the Bezier curve.
	</p>
	<div class="row justify-content-md-center text-center">
		<div class="col-md-6"><img class="img-fluid" src="img/t1_1.jpg" ><p>1.3 Step 0</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t1_2.jpg" ><p>1.3 Step 1</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t1_3.jpg" ><p>1.3 Step 2</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t1_4.jpg" ><p>1.3 Step 3</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t1_5.jpg" ><p>1.3 Step 4</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t1_6.jpg" ><p>1.3 Step 5</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t1_7.jpg" ><p>1.3 Step 6</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t2_1.jpg" ><p>1.4</p></div>
	</div>

</div>

<div class="container">
	<h2 id="task2">Part 2</h2>
	<p>
		2.1 The main problem is how we can extend de Casteljau's algorithm to a three dimension space. The idea is very simple. We first do this algorithm along one axis based on a process very similar to that
		adopted in evaluating Bezier curves acocording to the given u. After this we will obtain some intermediate points, then we redo this thing again but along the other axis according to the given v. Finally,
		we will obtain one point on the Bezier surfaces. To implement this, we just followed this idea to realize this algorithm. We first traversed each vector of control points, do de Casteljau's algorithm
		to get the intermediate point and put this point into a new vector. After traversing the whole original control points, we then processed the intermediate points in that new vector and did the algorithm
		again and finally obtained the final surface point.
	</p>

	<div class="row justify-content-md-center text-center">
		<div class="col-md-6"><img class="img-fluid" src="img/t2_2.jpg" ><p>bez/teapot.bez</p></div>

	</div>

</div>

<div class="container">
	<h2 id="task3">Part 3</h2>
	<p>
		3.1 To implement area-weighted vertex normals, after obtaining some halfedge rooted at the given vertex, we used twin() and next() to traverse all faces incident to that vertex and saved one halfedge of each face into
		a new vector. Then, we traversed all halfedge in that vector to get the normal of the corresponding face. About obtaining the normal of the face, we could call next() twice then we would get three consecutive
		halfedge in that face. Then we could access the three vertexes in that face and their position. Given these three vertexes, we could then simply obtain the normal and the area of that face. One thing worth mentioning is that
		we would do normalization on this normal vector. The last thing we did was weighing all these normals of faces by their corresponding areas and took the result as the final area-weighted vertex normal.
	</p>
	<div class="row justify-content-md-center text-center">
		<div class="col-md-6"><img class="img-fluid" src="img/t3_1.jpg" ><p>w/o vertex normals</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/t3_2.jpg" ><p>with vertex normals</p></div>
	</div>
</div>

<div class="container">
	<h2 id="task4">Part 4</h2>
	<p>
		4.1 We followed some suggestions mentioned in the instruction and the provided <a href="http://15462.courses.cs.cmu.edu/fall2015content/misc/HalfedgeEdgeOpImplementationGuide.pdf">guide</a>. Since we wouldn't add or delete any new elements.
		We first drew a diagram showing the elements before edge flip and that after edge flip. Then we collected all elements no matter whether they would be changed. According to the change found in
		the diagram, we then updated the pointers for all elements no matter whether they would be affected by the edge flip. Finally, we removed some unnecessary operations. Following this method, we succeeded
		implementing edge flip in one go.
	</p>
	<div class="row justify-content-md-center text-center">
		<div class="col-md-6"><img class="img-fluid" src="img/t4_1.jpg" ></div>
	</div>
</div>

<div class="container">
	<h2 id="task5">Task 5</h2>
	<p>
		5.1 Pixel sampling is to sample the texture value at a point based on the surrounding texture sample locations. In task 5, we will first use the given vertices to find the barcentric coordinates and interpolation matrics to translate between in screen coordinates and texture images.
		Then we will iterate through every pixels (x and y) in the supersampling buffers in the screen images, and conduct a barycentric transform to find its corresponding u, v values in the texture image. With the uv values between 0 and 1, we first multiply them by the texture image's width and height, which will give us its coordinates in texture image in double.
		Then could sample the colors at this point using either nearest sampling or bilinear sampling, and find the correct Color.
	</p>
	<p>
		5.2 The uv texture coordinate we found is likely to be float / double. Nearst sampling is to find the closest 1 texture pixel (with indices to be integers) to simulate the color of the current pixel. Bilinear sampling is to use the 4 surrounding texture sample locations and interpolating their colors together using
		lerp function.
	</p>
	<p>
		5.3 As we could see from the images below, nearest sampling will have more abrupt changes on the edges or when colors change, while bilinear interpolation will have smoother transition and less artifact. This is because in the nearest sampling, we only sample the screen pixels from 1 closest texture pixel, which has abrupt changes when colors change.
		However, with bilinear interpolation, we could blend in the all 4 colors from the surrounding pixels, with larger supersampling, we could blend in and interpolate even more surrounding texture changes. Therefore, bilinear interpolation will be less artifact.
	</p>
	<div class="row justify-content-md-center text-center">
		<div class="col-md-6"><img class="img-fluid" src="img/task5_nearest_1.png" ><p>Nearest Sample, 1 supersample</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/task5_bi_1.png" ><p>Bilinear Sample, 1 supersample</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/task5_nearest_16.png" ><p>Nearest Sample, 16 supersample</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/task5_bi_16.png" ><p>Bilinear Sample, 16 supersample</p></div>
	</div>
</div>

<div class="container">
	<h2 id="task6">Task 6</h2>
	<p>
		6.1 When we do texture minification, many texels can contribute to pixel footprint, which means we are sampling a high frequency signal at a low frequency. This would create some new random signals like aliasing. Thus, we need to apply a low-pass filter to the texture, removing high frequencies, and sample at a lower resolution to create a higher-level texture map.
		Doing this recursively, we will obtain the mipmap with different levels. In practice, we would first generate the mipmap first, then based on the sample frequency, or in other words, the degree of minimization, choose which level or levels of mipmap we would use, according to our mipmap level resampling choice, always level 0 or nearest D or linear interpolation.
	</p>
	<p>
		6.2 If we sample a lot of pixels, we would definitely have a very high performance, but this would take a very long time and would consume many memory. Level sampling would consume a lot of memory, since we need to save mipmap of all levels. However, once it's done, it would take few time to get the result and the antialiasing power would almost be as good as sampling a lot of pixels.
		Pixel sampling like bilinear filtering would also take a lot of time and memory, but is great for texture magnification.
	</p>
	<div class="row justify-content-md-center text-center">
		<div class="col-md-6"><img class="img-fluid" src="img/task6_1.png" ><p>L_ZERO and P_NEAREST</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/task6_2.png" ><p>L_ZERO and P_LINEAR</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/task6_3.png" ><p>L_NEAREST and P_NEAREST</p></div>
		<div class="col-md-6"><img class="img-fluid" src="img/task6_4.png" ><p>L_NEAREST and P_LINEAR</p></div>
	</div>
</div>

</body>
</html>